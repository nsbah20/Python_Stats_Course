{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dac1396",
   "metadata": {},
   "source": [
    "# Module 5: Advanced Regression Techniques\n",
    "In this module, we explore regularized regression methods and logistic regression â€” powerful tools for improving model generalization and handling classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9c8684",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Learning Objectives\n",
    "- Understand why and when to use Ridge and Lasso regression\n",
    "- Apply logistic regression for binary classification\n",
    "- Evaluate classification models with confusion matrix and ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c7da82",
   "metadata": {},
   "source": [
    "## ðŸ§® Ridge and Lasso Regression\n",
    "These are regularized versions of linear regression that add a penalty to the model coefficients to reduce overfitting:\n",
    "\n",
    "- **Ridge Regression** adds an L2 penalty (squared magnitude of coefficients)\n",
    "- **Lasso Regression** adds an L1 penalty (absolute value of coefficients)\n",
    "\n",
    "These methods help when you have multicollinearity or want to shrink irrelevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80265b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(100, 5)\n",
    "coefs = np.array([5, 0, 3, 0, 2])\n",
    "y = X @ coefs + np.random.normal(0, 1, 100)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Fit Ridge model\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "print(f\"Ridge RMSE: {mean_squared_error(y_test, y_pred_ridge, squared=False):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9538ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Lasso model\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "print(f\"Lasso RMSE: {mean_squared_error(y_test, y_pred_lasso, squared=False):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb4fbb",
   "metadata": {},
   "source": [
    "## ðŸ¤– Logistic Regression\n",
    "Logistic regression is used when the target variable is binary (e.g., yes/no, success/failure).\n",
    "\n",
    "Instead of predicting a value, it predicts the **probability** of class membership using the logistic (sigmoid) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create classification data\n",
    "X, y = make_classification(n_samples=100, n_features=4, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# Fit logistic model\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Accuracy: {clf.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd130bb",
   "metadata": {},
   "source": [
    "## ðŸ“Š Evaluating Classification Models\n",
    "- **Confusion Matrix** shows counts of true/false positives/negatives\n",
    "- **ROC Curve** plots true positive rate vs false positive rate at various thresholds\n",
    "- **AUC** is area under ROC curve (closer to 1 is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict probabilities\n",
    "probs = clf.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8a373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "preds = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505bfba",
   "metadata": {},
   "source": [
    "## âœ… Practice Exercises\n",
    "1. Simulate multicollinear data and compare Ridge vs Lasso performance.\n",
    "2. Use logistic regression to classify iris flower species (setosa vs non-setosa).\n",
    "3. Plot an ROC curve and calculate AUC.\n",
    "4. Reflect: What are the trade-offs of using regularization?\n",
    "5. Try adjusting `alpha` for Ridge and Lasso â€” how do the coefficients and RMSE change?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
