{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a2ecec",
   "metadata": {},
   "source": [
    "# Module 6: Ensemble Learning and Decision Trees\n",
    "In this module, we explore tree-based models ‚Äî simple and powerful tools for classification and regression ‚Äî and extend them with ensemble learning methods like Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10152f",
   "metadata": {},
   "source": [
    "## üéØ Learning Objectives\n",
    "- Understand how decision trees work\n",
    "- Learn the pros and cons of tree-based models\n",
    "- Apply and evaluate a Random Forest classifier\n",
    "- Interpret feature importance in ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0eaa2",
   "metadata": {},
   "source": [
    "## üå≥ Decision Trees\n",
    "A decision tree splits the dataset into branches based on conditions. At each node, it chooses the best feature that separates the data.\n",
    "\n",
    "Pros:\n",
    "- Easy to understand and interpret\n",
    "- Handles both numeric and categorical data\n",
    "\n",
    "Cons:\n",
    "- Prone to overfitting\n",
    "- Sensitive to small data changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(12, 6))\n",
    "plot_tree(clf, filled=True, feature_names=load_iris().feature_names, class_names=load_iris().target_names)\n",
    "plt.title('Decision Tree')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78904398",
   "metadata": {},
   "source": [
    "## üå≤ Random Forests\n",
    "Random Forests build multiple trees using different subsets of data and features, and combine their predictions for better performance.\n",
    "\n",
    "Advantages:\n",
    "- Reduces overfitting\n",
    "- Handles large datasets\n",
    "- Provides feature importance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c21bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1853a",
   "metadata": {},
   "source": [
    "## üîç Feature Importance\n",
    "Random Forests can tell us which features were most influential in making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78648c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = pd.Series(rf.feature_importances_, index=load_iris().feature_names)\n",
    "feature_importances.sort_values().plot(kind='barh')\n",
    "plt.title('Feature Importance in Random Forest')\n",
    "plt.xlabel('Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6449b810",
   "metadata": {},
   "source": [
    "## ‚úÖ Practice Exercises\n",
    "1. Train a decision tree on the `wine` or `breast cancer` dataset from `sklearn.datasets`.\n",
    "2. Try limiting the depth of the tree ‚Äî how does accuracy change?\n",
    "3. Use a Random Forest classifier and evaluate its performance.\n",
    "4. Plot and interpret feature importances.\n",
    "5. Reflect: Why are Random Forests less prone to overfitting than a single decision tree?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
